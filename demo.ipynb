{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# UCL-COMP0087 Project Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Colab Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup\n",
    "from google import colab\n",
    "colab.drive.mount('/content/drive')\n",
    "\n",
    "# all imports, login, connect drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from googleapiclient.discovery import build\n",
    "drive = build('drive', 'v3').files()\n",
    "\n",
    "# recursively get names\n",
    "def get_path(file_id):\n",
    "    f = drive.get(fileId=file_id, fields='name, parents').execute()\n",
    "    name = f.get('name')\n",
    "    if f.get('parents'):\n",
    "        parent_id = f.get('parents')[0]  # assume 1 parent\n",
    "        return get_path(parent_id) / name\n",
    "    else:\n",
    "        return Path(name)\n",
    "\n",
    "# change directory\n",
    "def chdir_notebook():\n",
    "    d = requests.get('http://172.28.0.2:9000/api/sessions').json()[0]\n",
    "    file_id = d['path'].split('=')[1]\n",
    "    path = get_path(file_id)\n",
    "    nb_dir = 'drive' / path.parent\n",
    "    os.chdir(nb_dir)\n",
    "    return nb_dir\n",
    "\n",
    "!cd /\n",
    "chdir_notebook()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r config/requirements_colab.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## A. DATA preprocessing\n",
    "### 1) JSON->bin\n",
    "Preprocess .json format Conala data and save processed .bin file to `data/conala` folder.\n",
    "\n",
    "**Data Format**\n",
    "* Source: Short StackOverflow natural language questions.\n",
    "* Target: Code snippet. e.x. `pandas.read('file.csv', nrows=100)`.\n",
    "\n",
    "**Preprocess**\n",
    "* Canonicalization: Identify question specific string / variable names using RegEx and replace them with universal tokens `str_0`, `str_1` and `var_0`, `var_1`, etc.\n",
    "* Lowercase, tokenization\n",
    "\n",
    "#TODO: Understand preprocessing details and note down here.\n",
    "\n",
    "**Data Split**\n",
    "* Scraped data: This set is not human-curated. Used for pretraining.\n",
    "* Train set: Human-curated training set for fine-tuning. In total 2185 gold data.\n",
    "* Dev set: Held out 200 dev examples from gold training set.\n",
    "* Test set: 500 Conala testing instances."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process gold training data...\r\n",
      "Skipped due to exceptions: 123\r\n",
      "use mined data:  200\r\n",
      "from file:  data/conala-corpus/conala-mined.jsonl\r\n",
      "Skipped due to exceptions: 8\r\n",
      "2248 training instances\r\n",
      "200 dev instances\r\n",
      "process testing data...\r\n",
      "Skipped due to exceptions: 34\r\n",
      "466 testing instances\r\n",
      "number of word types: 1613, number of word types w/ frequency > 1: 882\r\n",
      "number of singletons:  731\r\n",
      "number of words not included: 972\r\n",
      "total token count:  22658\r\n",
      "unk token count:  1213\r\n",
      "number of word types: 1749, number of word types w/ frequency > 1: 763\r\n",
      "number of singletons:  986\r\n",
      "number of words not included: 1272\r\n",
      "total token count:  14861\r\n",
      "unk token count:  1558\r\n",
      "number of word types: 1701, number of word types w/ frequency > 1: 758\r\n",
      "number of singletons:  943\r\n",
      "number of words not included: 1204\r\n",
      "total token count:  35988\r\n",
      "unk token count:  1465\r\n",
      "generated vocabulary Vocab(source Vocabulary[size=645]words, primitive Vocabulary[size=481]words, code Vocabulary[size=501]words)\r\n",
      "Max action len: 96\r\n",
      "Avg action len: 23\r\n",
      "Actions larger than 100: 0\r\n"
     ]
    }
   ],
   "source": [
    "mined_data_file = \"data/conala-corpus/conala-mined.jsonl\"\n",
    "topk = 200 # number of pretraining data to be preprocessed\n",
    "!python datasets/conala/dataset.py --pretrain=$mined_data_file --topk=$topk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:['concatenate', 'elements', 'of', 'a', 'list', 'str_0', 'of', 'multiple', 'integers', 'to', 'a', 'single', 'integer'] \n",
      "Target:sum(d * 10 ** i for i, d in enumerate(str_0[::-1])) \n",
      "\n",
      "Source:['convert', 'a', 'list', 'of', 'integers', 'into', 'a', 'single', 'integer'] \n",
      "Target:r = int(''.join(map(str, x))) \n",
      "\n",
      "Source:['convert', 'a', 'datetime', 'string', 'back', 'to', 'a', 'datetime', 'object', 'of', 'format', 'str_0'] \n",
      "Target:datetime.strptime('2010-11-13 10:33:54.227806', 'str_0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of processed data.\n",
    "from components.dataset import Dataset\n",
    "n_example = 3\n",
    "train_set = Dataset.from_bin_file(\"data/conala/train.gold.full.bin\")\n",
    "for src, tgt in zip(train_set.all_source[:n_example],train_set.all_targets[:n_example]):\n",
    "    print(f'Source:{src} \\nTarget:{tgt} \\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## Model\n",
    "To adhere to the syntax requirements of code snippets, we use coding language independent AST to guide our generation of code [TODO:citation].\n",
    "\n",
    "**Code <-> Series of Actions**\n",
    "* Target <-> Python AST <--asdl--> asdl AST <--> Action series.\n",
    "* Target: code snippet.\n",
    "* Python AST: Language dependent Abstract Syntax Tree.\n",
    "* asdl: Text file that specifies the Grammar of Python3.\n",
    "* asdl AST: Language independent Abstract Syntax Tree.\n",
    "* action series: Series of actions needed to generate an AST.\n",
    "\n",
    "TODO: some model graphs here? AST examples?\n",
    "\n",
    "**Source Sequence <-> Action Sequence**\n",
    "* Tranx baseline: LSTM <-> LSTM\n",
    "* TODO: our model, bert??\n",
    "\n",
    "**Technical Details**\n",
    "* Initialization: glorot_init vs. xavier_normal_ ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "use glorot initialization\n",
      "begin training, 500 training examples, 200 dev examples\n",
      "vocab: Vocab(source Vocabulary[size=629]words, primitive Vocabulary[size=464]words, code Vocabulary[size=486]words)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1] epoch elapsed 9s\n",
      "[Epoch 1] begin validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 200/200 [00:55<00:00,  3.60it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-96b3ee14f415>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mParameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_param\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0;34m'train'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;32melif\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0;34m'test'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mtest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/ucl_nlp/exp.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    150\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'[Epoch %d] begin validation'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstderr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    151\u001B[0m                 \u001B[0meval_start\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 152\u001B[0;31m                 eval_results = evaluation.evaluate(dev_set.examples, model, evaluator, args,\n\u001B[0m\u001B[1;32m    153\u001B[0m                                                    verbose=False, eval_top_pred_only=args.eval_top_pred_only)\n\u001B[1;32m    154\u001B[0m                 \u001B[0mdev_score\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval_results\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mevaluator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefault_metric\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Tranx baseline model\n",
    "!bash scripts/conala/train.sh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}