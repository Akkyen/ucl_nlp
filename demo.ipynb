{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# COMP0087 Group Project - Python Code Generator\n",
    "Code generation is becoming an important and trending field in natural language processing (NLP), as it could potentially help to improve programming productivity by developing automatic code. Given some natural language (NL) utterances, the code generator aims to output some source code that completes the task described in the NL intents. Many models for the code generation task have been proposed by the researchers. In particular, TranX is a transition-based neural abstract syntax parser for code generation, it achieves state-of-the-art results on the CoNaLa dataset.\n",
    "\n",
    "However, existing code generation models suffer from various problems. For example, TRANX often leads to disadvantageous performance when dealing with long and complex code generation tasks. Furthermore, current code generators suffer from learning dependencies between distant positions. In particular, TRANX uses standard bidirectional Long Short-term Memory (LSTM) network as the encoder and decoder, which may lead to this issue due to its sequential computation. TranX also has high complexity and high computational cost due to its recurrent layer type.\n",
    "\n",
    "To solve these problems, this project explores potential solutions by using TRANX as the baseline, experimenting and modifying the encoder with different networks like Gated Recurrent Units (GRUs) and attentional encoder. In particular, TRANX_GRU beats the TRANX baseline results in terms of the exact match on the CoNaLa dataset. TranX_attentional_encoder achieves similar results as TRANX in terms of Corpus BLUE score while giving lower computational complexity per layer.\n",
    "(hopefully) both candidate models beat the current state-of-the-art tranX model on conala dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## 1 System Architecture\n",
    "\n",
    "##########insert pic##########\n",
    "\n",
    "Figure 1 and 2 gives brief overview of the system for tranX_GRU and tranX_Transformer respectively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## 2 Project Setup\n",
    "This project can be either run on colab or the local machine. Please find the project set up in the corresponding subsection below. To run it without CUDA, please simply remove the \"--cuda\" flag from the command line argument in all shell scripts under the file named \"scripts\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Colab Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup\n",
    "from google import colab\n",
    "colab.drive.mount('/content/drive')\n",
    "\n",
    "# Imports, login, connect drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from googleapiclient.discovery import build\n",
    "drive = build('drive', 'v3').files()\n",
    "\n",
    "# Recursively get names\n",
    "def get_path(file_id):\n",
    "    f = drive.get(fileId=file_id, fields='name, parents').execute()\n",
    "    name = f.get('name')\n",
    "    if f.get('parents'):\n",
    "        parent_id = f.get('parents')[0]  # assume 1 parent\n",
    "        return get_path(parent_id) / name\n",
    "    else:\n",
    "        return Path(name)\n",
    "\n",
    "# Change directory\n",
    "def chdir_notebook():\n",
    "    d = requests.get('http://172.28.0.2:9000/api/sessions').json()[0]\n",
    "    file_id = d['path'].split('=')[1]\n",
    "    path = get_path(file_id)\n",
    "    nb_dir = 'drive' / path.parent\n",
    "    os.chdir(nb_dir)\n",
    "    return nb_dir\n",
    "\n",
    "!cd /\n",
    "chdir_notebook()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Local Machine Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clone our project repository into the local machine\n",
    "git clone https://github.com/kzCassie/ucl_nlp\n",
    "# Enter the project file\n",
    "cd ucl_nlp\n",
    "\n",
    "# Create virtual environments\n",
    "python3 -m venv config/env\n",
    "# Activate virtual environment\n",
    "source config/env/bin/activate\n",
    "# Install all the required packages\n",
    "pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## 3 Data Loading & Data Preprocessing\n",
    "\n",
    "Run the following shell script to get the Conala json file from http://www.phontron.com/download/conala-corpus-v1.1.zip, and download the preprocessed Conala zipfile from GoogleDrive."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download original Conala json dataset\n",
    "# Download pre-processed Colana zipfile from GoogleDrive\n",
    "!bash pull_data.sh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Clarification on Data Preprocessing**\n",
    "\n",
    "Please note the data were preprocessed with the downloaded mined file and topk=100000 (First k number from mined file) using the code below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mined_data_file = \"data/conala-corpus/conala-mined.jsonl\" # path to the downloaded mined file\n",
    "topk = 100000 # number of pretraining data to be preprocessed\n",
    "!python datasets/conala/dataset.py --pretrain=$mined_data_file --topk=$topk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We preprocess the json files into several bin files and save them to the folder named `data/canola/${topk}`. These preprocessed files are then used in the next section for training, fine-tuning and testing.\n",
    "\n",
    "In particular, we preprocess the mined json file (conala-mined.jsonl) and save the results into *mined_100000.bin*, which is then used for the model pre-training. Next, the gold training data are preprocessed with the downloaded train file (*conala-train.json*), these preprocessed files are stored in *train.gold.full.bin*, and they are used for fine-tuning. At the end, we preprocess the test json file *conala-test.json* to *test.bin* and use it for model testing. In total, we use around 100000, 2500 and 466 instances for training, fine-tuning and testing respectively.\n",
    "\n",
    "Please see the example of the preprocessed data below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:['concatenate', 'elements', 'of', 'a', 'list', 'str_0', 'of', 'multiple', 'integers', 'to', 'a', 'single', 'integer'] \n",
      "Target:sum(d * 10 ** i for i, d in enumerate(str_0[::-1])) \n",
      "\n",
      "Source:['convert', 'a', 'list', 'of', 'integers', 'into', 'a', 'single', 'integer'] \n",
      "Target:r = int(''.join(map(str, x))) \n",
      "\n",
      "Source:['convert', 'a', 'datetime', 'string', 'back', 'to', 'a', 'datetime', 'object', 'of', 'format', 'str_0'] \n",
      "Target:datetime.strptime('2010-11-13 10:33:54.227806', 'str_0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of pre-processed data.\n",
    "from components.dataset import Dataset\n",
    "n_example = 3\n",
    "train_set = Dataset.from_bin_file(\"data/conala/train.gold.full.bin\")\n",
    "for src, tgt in zip(train_set.all_source[:n_example],train_set.all_targets[:n_example]):\n",
    "    print(f'Source:{src} \\nTarget:{tgt} \\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## 4 Model Training & Fine-tuning\n",
    "\n",
    "### tranX_LSTM (Baseline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tranX baseline model\n",
    "# pretrain with mined_num = 100000\n",
    "！bash scripts/tranX/pretrain.sh 100000\n",
    "\n",
    "# fine-tune with mined_num = 100000\n",
    "! bash scripts/tranX/finetune.sh 100000\n",
    "\n",
    "# test with mined_num = 100000\n",
    "!bash scripts/tranX/test.sh 100000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tranX_GRU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GRU model\n",
    "# pretrain with mined_num = 100000\n",
    "！bash scripts/GRU/pretrain.sh 100000\n",
    "\n",
    "# fine-tune with mined_num = 100000\n",
    "! bash scripts/GRU/finetune.sh 100000\n",
    "\n",
    "# test with mined_num = 100000\n",
    "!bash scripts/GRU/test.sh 100000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tranX_Transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transformer\n",
    "# pretrain with mined_num = 100000\n",
    "！bash scripts/transformer/pretrain.sh 100000\n",
    "\n",
    "# fine-tune with mined_num = 100000\n",
    "! bash scripts/transformer/finetune.sh 100000\n",
    "\n",
    "# test with mined_num = 100000\n",
    "!bash scripts/transformer/test.sh 100000\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## 5 Model Testing with the test set provided in CoNaLa dataset\n",
    "\n",
    "### tranX_LSTM (Baseline)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tranX baseline model\n",
    "# test with mined_num = 100000\n",
    "!bash scripts/tranX/test.sh 100000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tranX_GRU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GRU model\n",
    "# test with mined_num = 100000\n",
    "!bash scripts/GRU/test.sh 100000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tranX_attentional_encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transformer\n",
    "# test with mined_num = 100000\n",
    "!bash scripts/transformer/test.sh 100000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "\n",
    "DRAFT - NOT BY ME\n",
    "## 4 Model\n",
    "To ad here to the syntax requirements of code snippets, we use coding language independent AST to guide our generation of code [TODO:citation].\n",
    "\n",
    "**Code <-> Series of Actions**\n",
    "* Target <-> Python AST <--asdl--> asdl AST <--> Action series.\n",
    "* Target: code snippet.\n",
    "* Python AST: Language dependent Abstract Syntax Tree.\n",
    "* asdl: Text file that specifies the Grammar of Python3.\n",
    "* asdl AST: Language independent Abstract Syntax Tree.\n",
    "* action series: Series of actions needed to generate an AST.\n",
    "\n",
    "TODO: some model graphs here? AST examples?\n",
    "\n",
    "**Source Sequence <-> Action Sequence**\n",
    "* Tranx baseline: LSTM <-> LSTM\n",
    "* TODO: our model, bert??\n",
    "\n",
    "**Technical Details**\n",
    "* Initialization: glorot_init vs. xavier_normal_ ?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
