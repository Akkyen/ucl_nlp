{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# UCL-COMP0087 Project Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup\n",
    "from google import colab\n",
    "colab.drive.mount('/content/drive')\n",
    "\n",
    "# all imports, login, connect drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from googleapiclient.discovery import build\n",
    "drive = build('drive', 'v3').files()\n",
    "\n",
    "# recursively get names\n",
    "def get_path(file_id):\n",
    "    f = drive.get(fileId=file_id, fields='name, parents').execute()\n",
    "    name = f.get('name')\n",
    "    if f.get('parents'):\n",
    "        parent_id = f.get('parents')[0]  # assume 1 parent\n",
    "        return get_path(parent_id) / name\n",
    "    else:\n",
    "        return Path(name)\n",
    "\n",
    "# change directory\n",
    "def chdir_notebook():\n",
    "    d = requests.get('http://172.28.0.2:9000/api/sessions').json()[0]\n",
    "    file_id = d['path'].split('=')[1]\n",
    "    path = get_path(file_id)\n",
    "    nb_dir = 'drive' / path.parent\n",
    "    os.chdir(nb_dir)\n",
    "    return nb_dir\n",
    "\n",
    "!cd /\n",
    "chdir_notebook()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## A. DATA preprocessing\n",
    "### 1) JSON->bin\n",
    "Preprocess .json format Conala data and save processed .bin file to `data/conala` folder.\n",
    "\n",
    "**Data Format**\n",
    "* Source: Short StackOverflow natural language questions.\n",
    "* Target: Code snippet. e.x. `pandas.read('file.csv', nrows=100)`.\n",
    "\n",
    "**Preprocess**\n",
    "* Canonicalization: Identify question specific string / variable names using RegEx and replace them with universal tokens `str_0`, `str_1` and `var_0`, `var_1`, etc.\n",
    "* Lowercase, tokenization\n",
    "\n",
    "#TODO: Understand preprocessing details and note down here.\n",
    "\n",
    "**Data Split**\n",
    "* Scraped data: This set is not human-curated. Used for pretraining.\n",
    "* Train set: Human-curated training set for fine-tuning. In total 2185 gold data.\n",
    "* Dev set: Held out 200 dev examples from Conala training set.\n",
    "* Test set: 500 Conala testing instances."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process gold training data...\r\n",
      "Skipped due to exceptions: 4\r\n",
      "use mined data:  10\r\n",
      "from file:  data/conala-corpus/conala-mined.jsonl\r\n",
      "Skipped due to exceptions: 0\r\n",
      "2185 training instances\r\n",
      "200 dev instances\r\n",
      "process testing data...\r\n",
      "Skipped due to exceptions: 0\r\n",
      "500 testing instances\r\n",
      "number of word types: 1611, number of word types w/ frequency > 1: 846\r\n",
      "number of singletons:  765\r\n",
      "number of words not included: 986\r\n",
      "total token count:  22275\r\n",
      "unk token count:  1207\r\n",
      "number of word types: 1709, number of word types w/ frequency > 1: 679\r\n",
      "number of singletons:  1030\r\n",
      "number of words not included: 1249\r\n",
      "total token count:  14410\r\n",
      "unk token count:  1468\r\n",
      "number of word types: 1670, number of word types w/ frequency > 1: 685\r\n",
      "number of singletons:  985\r\n",
      "number of words not included: 1188\r\n",
      "total token count:  34939\r\n",
      "unk token count:  1391\r\n",
      "generated vocabulary Vocab(source Vocabulary[size=629]words, primitive Vocabulary[size=464]words, code Vocabulary[size=486]words)\r\n",
      "Max action len: 96\r\n",
      "Avg action len: 23\r\n",
      "Actions larger than 100: 0\r\n"
     ]
    }
   ],
   "source": [
    "mined_data_file = \"data/conala-corpus/conala-mined.jsonl\"\n",
    "topk = 10 # number of pretraining data to be preprocessed\n",
    "!python datasets/conala/dataset.py --pretrain=$mined_data_file --topk=$topk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:['concatenate', 'elements', 'of', 'a', 'list', 'str_0', 'of', 'multiple', 'integers', 'to', 'a', 'single', 'integer'] \n",
      "Target:sum(d * 10 ** i for i, d in enumerate(str_0[::-1])) \n",
      "\n",
      "Source:['convert', 'a', 'list', 'of', 'integers', 'into', 'a', 'single', 'integer'] \n",
      "Target:r = int(''.join(map(str, x))) \n",
      "\n",
      "Source:['convert', 'a', 'datetime', 'string', 'back', 'to', 'a', 'datetime', 'object', 'of', 'format', 'str_0'] \n",
      "Target:datetime.strptime('2010-11-13 10:33:54.227806', 'str_0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of processed data.\n",
    "from components.dataset import Dataset\n",
    "n_example = 3\n",
    "train_set = Dataset.from_bin_file(\"data/conala/train.gold.full.bin\")\n",
    "for src, tgt in zip(train_set.all_source[:n_example],train_set.all_targets[:n_example]):\n",
    "    print(f'Source:{src} \\nTarget:{tgt} \\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## Model\n",
    "To adhere to the syntax requirements of code snippets, we use coding language independent AST to guide our generation of code.\n",
    "\n",
    "**Code <-> Series of Actions**\n",
    "* Target <-> Python AST <--asdl--> asdl AST <--> Action series.\n",
    "* Target: code snippet.\n",
    "* Python AST: Language dependent Abstract Syntax Tree.\n",
    "* asdl: Text file that specifies the Grammar of Python3.\n",
    "* asdl AST: Language independent Abstract Syntax Tree.\n",
    "* action series: Series of actions needed to generate the AST.\n",
    "\n",
    "TODO: some model graphs here? AST examples?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!bash scripts/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}